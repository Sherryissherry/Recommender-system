{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Computational Linguistics over Reddit Data\n",
    "\n",
    "For this project we are going to ingest Reddit posts, process the data and perform computational linguistics over the Reddit posts.\n",
    "\n",
    "This project will build off of some work you have previously done. However, beyond that exercise of processing and cataloging the feeds, in this instance you will access the referenced Reddit post and perform computational linguistics over the post itself.\n",
    "\n",
    "![DataScraper_To_NLP.png MISSING](../images/DataScraper_To_NLP.png)\n",
    "\n",
    "---\n",
    "\n",
    "### From the site:\n",
    "\n",
    "reddit: https://www.reddit.com/  \n",
    "Reddit gives you the best of the Internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos just for you.\n",
    "\n",
    "\n",
    "### From Wikipedia:\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. \n",
    "Registered members submit content to the site such as links, text posts, and images, \n",
    "which are then voted up or down by other members. \n",
    "Posts are organized by subject into user-created boards called \"subreddits\", \n",
    "which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. \n",
    "Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough votes, ultimately on the site's front page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Posting:\n",
    "\n",
    "The below link is an example post from someone that was tinkering with sentiment analysis; specifically they looked at the text of [Moby Dick](https://en.wikipedia.org/wiki/Moby-Dick).\n",
    "\n",
    "**Spoiler:** The conclusion was that the book is rather negative in sentiment.\n",
    "It is after all, about vengeance!\n",
    "\n",
    "https://www.reddit.com/r/LanguageTechnology/comments/9whk23/a_simple_nlp_pipeline_to_calculate_running/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From: https://www.redditinc.com/\n",
    "![REDDIT_About.png MISSING](../images/REDDIT_About_latest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example Code:\n",
    "\n",
    "In this exercise, we will be using Reddit API for fetching the latest messages. We can also fetch recent posts from Reddit using web feeds (check [here](./rss-feeds.ipynb)), but it seems our IP got banned for excessive requests to Reddit over the last few days. So we will be using Reddit API for which you are required to create your Reddit account and an app. \n",
    "\n",
    "Follow [this article](https://gilberttanner.com/blog/scraping-redditdata) to create your credentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reddit API\n",
    "\n",
    "For fetching Reddit data using API, we will be using a Python wrapper to Reddit API: [PRAW: The Python Reddit API Wrapper](https://github.com/praw-dev/praw)\n",
    "\n",
    "Documentation: https://praw.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='DTh3XVSfJnNEWII1fZ8x5g', \n",
    "                     client_secret='x3Hdxi6ZCc-wZsSMQyy70Tfabk-Z7w', \n",
    "                     user_agent='SherryRedditApp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024, Score: 2, URL: https://www.reddit.com/r/datascience/comments/1fxz22n/weekly_entering_transitioning_thread_07_oct_2024/\n",
      "Title: Transitioning into management, Score: 9, URL: https://www.reddit.com/r/datascience/comments/1g229sv/transitioning_into_management/\n",
      "Title: Where is that super informative thread that was a ton of information about how to get in Data Science, a background on what Data Scientists do, salary information, etc?, Score: 91, URL: https://www.reddit.com/r/datascience/comments/1g1icc7/where_is_that_super_informative_thread_that_was_a/\n",
      "Title: NHiTs: Deep Learning + Signal Processing for Time-Series Forecasting, Score: 2, URL: https://www.reddit.com/r/datascience/comments/1g239vr/nhits_deep_learning_signal_processing_for/\n",
      "Title: OpenAI Swarm for Multi-Agent Orchestration , Score: 4, URL: https://www.reddit.com/r/datascience/comments/1g1xphe/openai_swarm_for_multiagent_orchestration/\n",
      "Title: What do you consider to be the modern continuation of Deep Learning by Goodfellow?, Score: 14, URL: /r/learnmachinelearning/comments/1g1j0mm/modern_continuation_of_deep_learning_by_goodfellow/\n",
      "Title: Are AI models increasingly becoming more akin to a \"managed\" service like the cloud?, Score: 59, URL: https://www.reddit.com/r/datascience/comments/1g186jy/are_ai_models_increasingly_becoming_more_akin_to/\n",
      "Title: Graph analytics resources , Score: 14, URL: https://www.reddit.com/r/datascience/comments/1g1gecu/graph_analytics_resources/\n",
      "Title: Pyramid Flow free API for text-video, image-video generation , Score: 11, URL: https://www.reddit.com/r/datascience/comments/1g11jca/pyramid_flow_free_api_for_textvideo_imagevideo/\n",
      "Title: A Shiny app that writes shiny apps and runs them in your browser, Score: 117, URL: https://gallery.shinyapps.io/assistant/\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='DTh3XVSfJnNEWII1fZ8x5g',\n",
    "                     client_secret='x3Hdxi6ZCc-wZsSMQyy70Tfabk-Z7w',\n",
    "                     user_agent='SherryRedditApp')\n",
    "\n",
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "hot_posts = reddit.subreddit('datascience').hot(limit=10)  # hot posts\n",
    "\n",
    "# new_posts = reddit.subreddit('datascience').new(limit=10)  # new posts\n",
    "\n",
    "# get hottest posts from all subreddits\n",
    "# hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "\n",
    "for post in hot_posts:\n",
    "    print(f\"Title: {post.title}, Score: {post.score}, URL: {post.url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024, Score: 2, URL: https://www.reddit.com/r/datascience/comments/1fxz22n/weekly_entering_transitioning_thread_07_oct_2024/, Author: AutoModerator\n",
      "Title: Transitioning into management, Score: 8, URL: https://www.reddit.com/r/datascience/comments/1g229sv/transitioning_into_management/, Author: Hefty_Raisin_1473\n",
      "Title: Where is that super informative thread that was a ton of information about how to get in Data Science, a background on what Data Scientists do, salary information, etc?, Score: 89, URL: https://www.reddit.com/r/datascience/comments/1g1icc7/where_is_that_super_informative_thread_that_was_a/, Author: csingleton1993\n",
      "Title: NHiTs: Deep Learning + Signal Processing for Time-Series Forecasting, Score: 2, URL: https://www.reddit.com/r/datascience/comments/1g239vr/nhits_deep_learning_signal_processing_for/, Author: nkafr\n",
      "Title: OpenAI Swarm for Multi-Agent Orchestration , Score: 4, URL: https://www.reddit.com/r/datascience/comments/1g1xphe/openai_swarm_for_multiagent_orchestration/, Author: mehul_gupta1997\n",
      "Title: What do you consider to be the modern continuation of Deep Learning by Goodfellow?, Score: 14, URL: /r/learnmachinelearning/comments/1g1j0mm/modern_continuation_of_deep_learning_by_goodfellow/, Author: NDVGuy\n",
      "Title: Are AI models increasingly becoming more akin to a \"managed\" service like the cloud?, Score: 60, URL: https://www.reddit.com/r/datascience/comments/1g186jy/are_ai_models_increasingly_becoming_more_akin_to/, Author: LyleLanleysMonorail\n",
      "Title: Graph analytics resources , Score: 14, URL: https://www.reddit.com/r/datascience/comments/1g1gecu/graph_analytics_resources/, Author: ergodym\n",
      "Title: Pyramid Flow free API for text-video, image-video generation , Score: 11, URL: https://www.reddit.com/r/datascience/comments/1g11jca/pyramid_flow_free_api_for_textvideo_imagevideo/, Author: mehul_gupta1997\n",
      "Title: A Shiny app that writes shiny apps and runs them in your browser, Score: 115, URL: https://gallery.shinyapps.io/assistant/, Author: IntelligentDust6249\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='DTh3XVSfJnNEWII1fZ8x5g',\n",
    "                     client_secret='x3Hdxi6ZCc-wZsSMQyy70Tfabk-Z7w',\n",
    "                     user_agent='SherryRedditApp')\n",
    "\n",
    "hot_posts = reddit.subreddit('datascience').hot(limit=10)\n",
    "\n",
    "all_posts = list(hot_posts)\n",
    "\n",
    "for post in all_posts:\n",
    "    print(f\"Title: {post.title}, Score: {post.score}, URL: {post.url}, Author: {post.author}\")\n",
    "\n",
    "# this line will initiate the fetching of posts as PRAW use a lazy approach (i.e, fetch when required)\n",
    "# this part is done to avoid calling Reddit API multiple times while developing our code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1fxz22n\n",
      "title : Weekly Entering & Transitioning - Thread 07 Oct, 2024 - 14 Oct, 2024\n",
      "url : https://www.reddit.com/r/datascience/comments/1fxz22n/weekly_entering_transitioning_thread_07_oct_2024/\n",
      "author : AutoModerator <class 'str'>\n",
      "score : 2 <class 'int'> \n",
      "subreddit : datascience <class 'praw.models.reddit.subreddit.Subreddit'> \n",
      "num_comments : 70\n",
      "body :  \n",
      "\n",
      "Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n",
      "\n",
      "* Learning resources (e.g. books, tutorials, videos)\n",
      "* Traditional education (e.g. schools, degrees, electives)\n",
      "* Alternative education (e.g. online courses, bootcamps)\n",
      "* Job search questions (e.g. resumes, applying, career prospects)\n",
      "* Elementary questions (e.g. where to start, what next)\n",
      "\n",
      "While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
      "created : 1728273691.0\n",
      "link_flair_text : None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for post in all_posts:\n",
    "    print(f\"id : {post.id}\")\n",
    "    print(f\"title : {post.title}\")\n",
    "    print(f\"url : {post.url}\")\n",
    "    print(f\"author : {str(post.author)} {type(str(post.author))}\")\n",
    "    print(f\"score : {post.score} {type(post.score)} \")\n",
    "    print(f\"subreddit : {post.subreddit} {type(post.subreddit)} \")\n",
    "    print(f\"num_comments : {post.num_comments}\")\n",
    "    print(f\"body : {post.selftext}\")\n",
    "    print(f\"created : {post.created}\")\n",
    "    print(f\"link_flair_text : {post.link_flair_text}\")\n",
    "    break  # break the loop after printing information about the first post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Reddits\n",
    "\n",
    "As described above, sub-reddits are communities organized around particular topics.\n",
    "\n",
    "Some example sub-reddits:\n",
    " * https://www.reddit.com/r/datascience/\n",
    " * https://www.reddit.com/r/MachineLearning/\n",
    " * https://www.reddit.com/r/LanguageTechnology/\n",
    " * https://www.reddit.com/r/NLP/\n",
    " * https://www.reddit.com/r/Python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Tasks\n",
    "\n",
    "## Part I: Data Acquisition and Loading \n",
    "1. Choose a subreddit of your choice. Preferably something of interest to you. \n",
    "1. Conceptualize a database design that can collect the data.\n",
    "    * Make sure your items (posts) are unique and not duplicated!\n",
    "    * Make sure you capture at least title, author, subreddit, tags, title link, and timestamp\n",
    "    * Along with the metadata, capture all the text into one or more data field(s) suitable for information retrieval\n",
    "    * Write triggers for auto updates of IR related fields\n",
    "    * Add index (either GIN or GiST) for the IR related fields\n",
    "    * Additionally, design a field to hold:\n",
    "        * Sentiment\n",
    "1. Implement the database in your PostgreSQL schema\n",
    "1. Implement cells of Python Code that \n",
    "    * collect the latest posts from a subreddit of your choice (**should be text-dominant not image/video**) and collect at least 500 posts (if possible), \n",
    "    * processes the messages to extract metadata, \n",
    "    * process the text for IR, and \n",
    "    * perform computational linguistics (i.e, extract sentiment scores), \n",
    "    * then insert the data into your database.\n",
    "1. After you have loaded data from a subreddit, choose a few more subreddits and load those!\n",
    "\n",
    "## Part II: Analytics \n",
    "\n",
    "1. Write some test queries following the text vectors from Module 7.\n",
    "1. Produce **interesting visualizations** of the linguistic data.\n",
    "    * Try to look for trends (within a subreddit) and and variations of topics across subreddits\n",
    "    * Some comparative plots across feeds\n",
    "1. Write a summary of your findings!\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data Acquisition and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Design your database\n",
    "\n",
    "Conceptualize a database design that can collect the data.\n",
    "* Make sure your items (posts) are unique and not duplicated!\n",
    "* Make sure you capture at least title, link, author, subreddit, tag/flair, and timestamp\n",
    "* Capture all the body text into fields suitable for information retrieval\n",
    "* Write triggers for auto updates of IR related fields\n",
    "* Add index (either GIN or GiST) for the IR related fields\n",
    "* Additionally, design a field to hold:\n",
    "    - Sentiment\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Your Design here. You can describe your design with text or picture\n",
    "## ---------------------------------------------------------------------------\n",
    "\n",
    "I will build a database table that can store Reddit posts for metadata and analytics information about Reddit posts. The table will contain the following fields: post_id, title, author, subreddit, url, tags, um_comments, score, body, created, sentiment_score. for information retrieval, the title and body fields will be optimized using either GIN or GiST indexing optimized, and triggers will be set up to automatically update the sentiment analysis fields when data is inserted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 2: Implement the database in your PostgreSQL schema\n",
    "\n",
    "You can choose any of the three ways to implement your database. \n",
    "\n",
    "* sql magic \n",
    "* sql terminal \n",
    "* psycopg2 or sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djkgg········\n",
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "# Initialize some variables\n",
    "mysso= \"djkgg\"   # this is also your schema name. \n",
    "schema= \"djkgg\" \n",
    "hostname='pgsql.dsa.lan'\n",
    "database='dsa_student'\n",
    "\n",
    "mypasswd = getpass.getpass(\"djkgg\")\n",
    "connection_string = f\"postgres://{mysso}:{mypasswd}@{hostname}/{database}\"\n",
    "    \n",
    "%load_ext sql\n",
    "%sql $connection_string \n",
    "\n",
    "# Then remove the password from computer memory\n",
    "del mypasswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://djkgg:***@pgsql.dsa.lan/dsa_student\n",
      "   postgres://dsa_ro_user:***@pgsql.dsa.lan/dsa_student\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE reddit_posts (\n",
    "    post_id VARCHAR PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    author VARCHAR,\n",
    "    subreddit VARCHAR,\n",
    "    url TEXT,\n",
    "    tags TEXT,\n",
    "    score INT,\n",
    "    num_comments INT,\n",
    "    body TEXT,\n",
    "    created TIMESTAMP,\n",
    "    sentiment_score FLOAT\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement cells of Python Code that\n",
    "\n",
    "* collect the latest posts from a subreddit of your choice (should be text-dominant not image/video) and collect at least 500 posts (if possible),\n",
    "* processes the messages to extract id, title, link, author, subreddit, tag/flair, timestamp, etc. \n",
    "* process the text for IR, and\n",
    "* perform computational linguistics (e.g., get sentiment scores)\n",
    "* then insert the data into your database.\n",
    "\n",
    "\n",
    "Notes: \n",
    "* Each call to Reddit API returns 100 entries max. If we set a limit of more than 100, PRAW will handle multiple API calls internally and lazily fetches data. Check obfuscation and API limitation in https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html. \n",
    "* Develop and test your code with less than 100 messages from a subreddit. Then increase the limit and add few more subreddits. \n",
    "* While loading the table, test with one row \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from vaderSentiment) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "data insert completed\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import praw\n",
    "import psycopg2\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "reddit = praw.Reddit(client_id='DTh3XVSfJnNEWII1fZ8x5g', \n",
    "                     client_secret='x3Hdxi6ZCc-wZsSMQyy70Tfabk-Z7w', \n",
    "                     user_agent='SherryRedditApp')\n",
    "\n",
    "subreddit = reddit.subreddit('datascience')\n",
    "hot_posts = subreddit.hot(limit=100)\n",
    "\n",
    "posts_data = []\n",
    "for post in hot_posts:\n",
    "    sentiment = analyzer.polarity_scores(post.title)['compound']  \n",
    "    posts_data.append({\n",
    "        'post_id': post.id,\n",
    "        'title': post.title,\n",
    "        'author': str(post.author),\n",
    "        'subreddit': post.subreddit.display_name,\n",
    "        'url': post.url,\n",
    "        'score': post.score,\n",
    "        'num_comments': post.num_comments,\n",
    "        'body': post.selftext,\n",
    "        'created': post.created_utc,\n",
    "        'sentiment_score': sentiment\n",
    "    })\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"dsa_student\",\n",
    "    user=\"djkgg\",\n",
    "    password=getpass.getpass(), \n",
    "    host=\"pgsql.dsa.lan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO reddit_posts (post_id, title, author, subreddit, url, score, num_comments, body, created, sentiment_score)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, to_timestamp(%s), %s)\n",
    "    ON CONFLICT (post_id) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "for post in posts_data:\n",
    "    cursor.execute(insert_query, (\n",
    "        post['post_id'], post['title'], post['author'], post['subreddit'], post['url'],\n",
    "        post['score'], post['num_comments'], post['body'], post['created'], post['sentiment_score']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"data insert completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: After you have loaded data from a subreddit, choose a few more subreddit and load those!\n",
    "\n",
    "Add cells if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "Subreddit datascience data insert successful\n",
      "Subreddit MachineLearning data insert successful\n",
      "data insert completed\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import praw\n",
    "import psycopg2\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "reddit = praw.Reddit(client_id='DTh3XVSfJnNEWII1fZ8x5g', \n",
    "                     client_secret='x3Hdxi6ZCc-wZsSMQyy70Tfabk-Z7w', \n",
    "                     user_agent='SherryRedditApp')\n",
    "\n",
    "subreddits = ['datascience', 'MachineLearning'] \n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"dsa_student\",\n",
    "    user=\"djkgg\",\n",
    "    password=getpass.getpass(),\n",
    "    host=\"pgsql.dsa.lan\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO reddit_posts (post_id, title, author, subreddit, url, score, num_comments, body, created, sentiment_score)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, to_timestamp(%s), %s)\n",
    "    ON CONFLICT (post_id) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        hot_posts = subreddit.hot(limit=100) \n",
    "\n",
    "        posts_data = []\n",
    "        for post in hot_posts:\n",
    "            sentiment = analyzer.polarity_scores(post.title)['compound']  \n",
    "            posts_data.append({\n",
    "                'post_id': post.id,\n",
    "                'title': post.title,\n",
    "                'author': str(post.author),\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'url': post.url,\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'body': post.selftext,\n",
    "                'created': post.created_utc,\n",
    "                'sentiment_score': sentiment\n",
    "            })\n",
    "\n",
    "        for post in posts_data:\n",
    "            cursor.execute(insert_query, (\n",
    "                post['post_id'], post['title'], post['author'], post['subreddit'], post['url'],\n",
    "                post['score'], post['num_comments'], post['body'], post['created'], post['sentiment_score']\n",
    "            ))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Subreddit {subreddit_name} data insert successful\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Subreddit {subreddit_name} processing error: {e}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"data insert completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In part II, we will search your database as `dsa_ro_user user`. To prepare your DB to be read, you will need to grant the dsa_ro_user schema access and select privileges on your table.\n",
    "\n",
    "```SQL\n",
    "GRANT USAGE ON SCHEMA <your schema> TO dsa_ro_user;  -- NOTE: change to your schema\n",
    "GRANT SELECT ON <your table> TO dsa_ro_user;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
